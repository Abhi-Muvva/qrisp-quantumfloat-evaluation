{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72e2e793",
   "metadata": {},
   "source": [
    "k-Nearest Neighbors classifier using a QuantumFloat Hamming-distance engine.\n",
    "\n",
    "This script:\n",
    "    - loads the Iris dataset (petal length & width),\n",
    "    - performs a train/test split,\n",
    "    - sweeps precision msize for integer quantization,\n",
    "    - runs brute-force kNN with a QuantumFloat-based Hamming-distance kernel,\n",
    "    - aggregates timing and qubit metrics,\n",
    "    - estimates circuit resources per distance call for each precision,\n",
    "    - saves metrics as a CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c501460e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from qfloat_hamming import (\n",
    "    load_iris_two_features_scaled,\n",
    "    hamming_distance_trainpoint,\n",
    "    estimate_resources_per_distance_call,\n",
    ")\n",
    "\n",
    "BASE_OUT_DIR = \"Outputs\"\n",
    "os.makedirs(BASE_OUT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f758a657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qrisp_knn_hamming(\n",
    "    X_test: np.ndarray,\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    msize: int,\n",
    "    k: int = 3,\n",
    "    shots: int = 512,\n",
    "    debug: bool = True,\n",
    "    compute_resources: bool = True,\n",
    ") -> Tuple[np.ndarray, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Brute-force kNN classifier using a QuantumFloat-based Hamming-distance engine.\n",
    "\n",
    "    For each test point, the function:\n",
    "        - quantizes the test and train features into integers in [0, 2^msize - 1],\n",
    "        - computes the Hamming distance to each train sample using\n",
    "          hamming_distance_trainpoint,\n",
    "        - selects the majority label among the k nearest neighbors.\n",
    "\n",
    "    Args:\n",
    "        X_test:\n",
    "            Array of shape (n_test, 2) with scaled test features.\n",
    "\n",
    "        X_train:\n",
    "            Array of shape (n_train, 2) with scaled train features.\n",
    "\n",
    "        y_train:\n",
    "            Array of shape (n_train,) with class labels in {0, 1, 2}.\n",
    "\n",
    "        msize:\n",
    "            Number of bits for integer quantization per scalar feature.\n",
    "\n",
    "        k:\n",
    "            Number of neighbors to consider for the majority vote.\n",
    "\n",
    "        shots:\n",
    "            Number of measurement shots for the distance register.\n",
    "\n",
    "        debug:\n",
    "            If True, prints per-sample progress logs.\n",
    "\n",
    "        compute_resources:\n",
    "            If True, builds and compiles a representative distance circuit once\n",
    "            per msize and adds gate-depth metrics to the returned metrics.\n",
    "\n",
    "    Returns:\n",
    "        y_pred:\n",
    "            Array of shape (n_test,) with predicted class labels.\n",
    "\n",
    "        metrics:\n",
    "            Dictionary with aggregate timing, qubit, and resource metrics\n",
    "            for this precision.\n",
    "    \"\"\"\n",
    "    n_test = X_test.shape[0]\n",
    "    n_train = X_train.shape[0]\n",
    "\n",
    "    scale = (2 ** msize) - 1\n",
    "    X_test_int = np.round(X_test * scale).astype(int)\n",
    "    X_train_int = np.round(X_train * scale).astype(int)\n",
    "\n",
    "    y_pred = np.zeros(n_test, dtype=int)\n",
    "\n",
    "    total_encoding_time = 0.0\n",
    "    total_encoding_qubits = 0.0\n",
    "    total_classification_time = 0.0\n",
    "    max_total_qubits = 0.0\n",
    "    num_distance_calls = 0\n",
    "\n",
    "    for i in range(n_test):\n",
    "        x_int = X_test_int[i]\n",
    "        distances: List[Tuple[int, int]] = []\n",
    "\n",
    "        if debug:\n",
    "            print(\n",
    "                f\"[qrisp][kNN-Hamming][msize={msize}] \"\n",
    "                f\"test sample {i+1}/{n_test}: x={X_test[i]}\"\n",
    "            )\n",
    "\n",
    "        for j in range(n_train):\n",
    "            train_vec_int = X_train_int[j]\n",
    "            label_j = int(y_train[j])\n",
    "\n",
    "            d_ham, meta = hamming_distance_trainpoint(\n",
    "                test_vec_int=x_int,\n",
    "                train_vec_int=train_vec_int,\n",
    "                msize=msize,\n",
    "                shots=shots,\n",
    "            )\n",
    "\n",
    "            distances.append((d_ham, label_j))\n",
    "\n",
    "            total_encoding_time += meta[\"encoding_time\"]\n",
    "            total_encoding_qubits += meta[\"encoding_qubits\"]\n",
    "            total_classification_time += meta[\"total_time\"]\n",
    "            max_total_qubits = max(max_total_qubits, meta[\"total_qubits\"])\n",
    "            num_distance_calls += 1\n",
    "\n",
    "        distances.sort(key=lambda pair: pair[0])\n",
    "        k_nearest = distances[:k]\n",
    "        labels_k = [lbl for (_, lbl) in k_nearest]\n",
    "        counts = np.bincount(labels_k, minlength=3)\n",
    "        y_pred[i] = int(np.argmax(counts))\n",
    "\n",
    "    num_scalar_encodes = num_distance_calls * 4.0\n",
    "\n",
    "    avg_encoding_time_per_feature = total_encoding_time / num_scalar_encodes\n",
    "    avg_encoding_qubits_per_call = total_encoding_qubits / num_distance_calls\n",
    "    avg_encoding_qubits_per_feature = avg_encoding_qubits_per_call / 4.0\n",
    "    avg_classification_time_per_sample = total_classification_time / n_test\n",
    "    avg_encoding_time_per_sample = total_encoding_time / n_test\n",
    "\n",
    "    metrics: Dict[str, float] = {\n",
    "        \"msize\": float(msize),\n",
    "        \"k\": float(k),\n",
    "        \"avg_encoding_time_per_feature\": float(avg_encoding_time_per_feature),\n",
    "        \"avg_encoding_qubits_per_call\": float(avg_encoding_qubits_per_call),\n",
    "        \"avg_encoding_qubits_per_feature\": float(avg_encoding_qubits_per_feature),\n",
    "        \"avg_classification_time_per_sample\": float(avg_classification_time_per_sample),\n",
    "        \"avg_encoding_time_per_sample\": float(avg_encoding_time_per_sample),\n",
    "        \"max_total_qubits_per_distance_call\": float(max_total_qubits),\n",
    "        \"num_test_samples\": float(n_test),\n",
    "        \"num_train_samples\": float(n_train),\n",
    "        \"num_distance_calls\": float(num_distance_calls),\n",
    "    }\n",
    "\n",
    "    if compute_resources:\n",
    "        res = estimate_resources_per_distance_call(msize)\n",
    "        metrics.update(res)\n",
    "\n",
    "    return y_pred, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05884aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    X_scaled, y_full, feature_names, target_names = load_iris_two_features_scaled()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled,\n",
    "        y_full,\n",
    "        test_size=0.25,\n",
    "        stratify=y_full,\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    print(f\"Train size: {X_train.shape[0]}, Test size: {X_test.shape[0]}\")\n",
    "    print(f\"Target names: {target_names}\\n\")\n",
    "\n",
    "    msizes = [3, 4, 5, 6, 7, 8]\n",
    "    k = 3\n",
    "\n",
    "    metrics_rows: List[Dict[str, float]] = []\n",
    "\n",
    "    for msize in msizes:\n",
    "        print(f\"Running Qrisp kNN (Hamming) with QuantumFloat msize={msize}, k={k}\")\n",
    "\n",
    "        y_pred, metrics = qrisp_knn_hamming(\n",
    "            X_test=X_test,\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            msize=msize,\n",
    "            k=k,\n",
    "            shots=512,\n",
    "            debug=True,\n",
    "            compute_resources=True,\n",
    "        )\n",
    "\n",
    "        cm = confusion_matrix(y_test, y_pred, labels=[0, 1, 2])\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        print(f\"  Accuracy (msize={msize}): {acc:.3f}\")\n",
    "        print(\"  Confusion matrix (rows=true, cols=pred):\")\n",
    "        print(cm)\n",
    "\n",
    "        cm_df = pd.DataFrame(\n",
    "            cm,\n",
    "            index=[f\"true_{cls}\" for cls in target_names],\n",
    "            columns=[f\"pred_{cls}\" for cls in target_names],\n",
    "        )\n",
    "        cm_path = os.path.join(BASE_OUT_DIR, f\"confusion_knn_precision_{msize}.csv\")\n",
    "        cm_df.to_csv(cm_path, float_format=\"%.6e\")\n",
    "\n",
    "        metrics_row: Dict[str, float] = {\n",
    "            \"msize\": float(msize),\n",
    "            \"k\": float(k),\n",
    "            \"accuracy\": float(acc),\n",
    "        }\n",
    "        metrics_row.update(metrics)\n",
    "        metrics_rows.append(metrics_row)\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics_rows)\n",
    "    metrics_csv_path = os.path.join(\n",
    "        BASE_OUT_DIR, \"knn_metrics_and_resources_by_precision.csv\"\n",
    "    )\n",
    "    metrics_df.to_csv(metrics_csv_path, index=False, float_format=\"%.6e\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
