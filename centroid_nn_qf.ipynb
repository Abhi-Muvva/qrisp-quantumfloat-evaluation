{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9823a843",
   "metadata": {},
   "source": [
    "Centroid-based classifier using a QuantumFloat Hamming-distance engine.\n",
    "\n",
    "This script:\n",
    "    - loads the Iris dataset (petal length & width),\n",
    "    - computes class centroids in the scaled 2D feature space,\n",
    "    - sweeps precision msize for integer quantization,\n",
    "    - classifies each sample by nearest centroid using a Hamming-distance\n",
    "      QuantumFloat kernel,\n",
    "    - aggregates timing and qubit metrics,\n",
    "    - estimates circuit resources per distance call for each precision,\n",
    "    - saves confusion matrices, metrics, and per-precision predictions as CSVs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545b3aa9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5797f93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "from qfloat_hamming import (\n",
    "    load_iris_two_features_scaled,\n",
    "    hamming_distance_trainpoint,\n",
    "    estimate_resources_per_distance_call,\n",
    ")\n",
    "\n",
    "BASE_OUT_DIR = \"Outputs\"\n",
    "os.makedirs(BASE_OUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db6281b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_centroids(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Compute class centroids in the 2D feature space.\n",
    "\n",
    "    Args:\n",
    "        X:\n",
    "            Array of shape (n_samples, 2) with scaled features.\n",
    "\n",
    "        y:\n",
    "            Array of shape (n_samples,) with class labels in {0, 1, 2}.\n",
    "\n",
    "    Returns:\n",
    "        centroids:\n",
    "            Array of shape (n_classes, 2) with per-class mean features.\n",
    "\n",
    "        class_labels:\n",
    "            Array with the distinct class labels used in centroids.\n",
    "    \"\"\"\n",
    "    classes = np.unique(y)\n",
    "    centroids: List[np.ndarray] = []\n",
    "\n",
    "    for cls in classes:\n",
    "        X_cls = X[y == cls]\n",
    "        mu_cls = X_cls.mean(axis=0)\n",
    "        centroids.append(mu_cls)\n",
    "\n",
    "    centroids_arr = np.asarray(centroids)\n",
    "    return centroids_arr, classes\n",
    "\n",
    "\n",
    "def centroid_nn_hamming(\n",
    "    X_test: np.ndarray,\n",
    "    centroids: np.ndarray,\n",
    "    centroid_labels: np.ndarray,\n",
    "    msize: int,\n",
    "    shots: int = 512,\n",
    "    debug: bool = True,\n",
    "    compute_resources: bool = True,\n",
    ") -> Tuple[np.ndarray, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Nearest-centroid classifier using a QuantumFloat-based Hamming-distance engine.\n",
    "\n",
    "    For each test point, the function:\n",
    "        - quantizes the test point and class centroids into integers in\n",
    "          [0, 2^msize - 1],\n",
    "        - computes the Hamming distance to each centroid using\n",
    "          hamming_distance_trainpoint,\n",
    "        - assigns the label of the nearest centroid.\n",
    "\n",
    "    Args:\n",
    "        X_test:\n",
    "            Array of shape (n_test, 2) with scaled test features.\n",
    "\n",
    "        centroids:\n",
    "            Array of shape (n_centroids, 2) with class centroids in scaled space.\n",
    "\n",
    "        centroid_labels:\n",
    "            Array of shape (n_centroids,) giving the label for each centroid.\n",
    "\n",
    "        msize:\n",
    "            Number of bits for integer quantization per scalar feature.\n",
    "\n",
    "        shots:\n",
    "            Number of measurement shots for the distance register.\n",
    "\n",
    "        debug:\n",
    "            If True, prints per-sample progress.\n",
    "\n",
    "        compute_resources:\n",
    "            If True, builds and compiles a representative distance circuit once\n",
    "            per msize and adds gate-depth metrics to the returned metrics.\n",
    "\n",
    "    Returns:\n",
    "        y_pred:\n",
    "            Array of shape (n_test,) with predicted class labels.\n",
    "\n",
    "        metrics:\n",
    "            Dictionary with aggregate timing, qubit, and resource metrics\n",
    "            for this precision.\n",
    "    \"\"\"\n",
    "    n_test = X_test.shape[0]\n",
    "    n_centroids = centroids.shape[0]\n",
    "\n",
    "    scale = (2 ** msize) - 1\n",
    "    X_test_int = np.round(X_test * scale).astype(int)\n",
    "    centroids_int = np.round(centroids * scale).astype(int)\n",
    "\n",
    "    y_pred = np.empty(n_test, dtype=int)\n",
    "\n",
    "    total_encoding_time = 0.0\n",
    "    total_encoding_qubits = 0.0\n",
    "    total_classification_time = 0.0\n",
    "    max_total_qubits = 0.0\n",
    "    num_distance_calls = 0\n",
    "\n",
    "    for i in range(n_test):\n",
    "        x_int = X_test_int[i]\n",
    "        distances: List[Tuple[int, int]] = []\n",
    "\n",
    "        if debug:\n",
    "            print(\n",
    "                f\"[qrisp][Centroid-Hamming][msize={msize}] \"\n",
    "                f\"test sample {i+1}/{n_test}: x={X_test[i]}\"\n",
    "            )\n",
    "\n",
    "        for centroid_vec_float, centroid_vec_int, label_k in zip(\n",
    "            centroids,\n",
    "            centroids_int,\n",
    "            centroid_labels,\n",
    "        ):\n",
    "            d_ham, meta = hamming_distance_trainpoint(\n",
    "                test_vec_int=x_int,\n",
    "                train_vec_int=centroid_vec_int,\n",
    "                msize=msize,\n",
    "                shots=shots,\n",
    "            )\n",
    "\n",
    "            distances.append((d_ham, int(label_k)))\n",
    "\n",
    "            total_encoding_time += meta[\"encoding_time\"]\n",
    "            total_encoding_qubits += meta[\"encoding_qubits\"]\n",
    "            total_classification_time += meta[\"total_time\"]\n",
    "            max_total_qubits = max(max_total_qubits, meta[\"total_qubits\"])\n",
    "            num_distance_calls += 1\n",
    "\n",
    "        distances.sort(key=lambda pair: pair[0])\n",
    "        best_distance, best_label = distances[0]\n",
    "        y_pred[i] = best_label\n",
    "\n",
    "    num_scalar_encodes = num_distance_calls * 4.0\n",
    "\n",
    "    avg_encoding_time_per_feature = total_encoding_time / num_scalar_encodes\n",
    "    avg_encoding_qubits_per_call = total_encoding_qubits / num_distance_calls\n",
    "    avg_encoding_qubits_per_feature = avg_encoding_qubits_per_call / 4.0\n",
    "    avg_classification_time_per_sample = total_classification_time / n_test\n",
    "    avg_encoding_time_per_sample = total_encoding_time / n_test\n",
    "\n",
    "    metrics: Dict[str, float] = {\n",
    "        \"msize\": float(msize),\n",
    "        \"avg_encoding_time_per_feature\": float(avg_encoding_time_per_feature),\n",
    "        \"avg_encoding_qubits_per_call\": float(avg_encoding_qubits_per_call),\n",
    "        \"avg_encoding_qubits_per_feature\": float(avg_encoding_qubits_per_feature),\n",
    "        \"avg_classification_time_per_sample\": float(avg_classification_time_per_sample),\n",
    "        \"avg_encoding_time_per_sample\": float(avg_encoding_time_per_sample),\n",
    "        \"max_total_qubits_per_distance_call\": float(max_total_qubits),\n",
    "        \"num_test_samples\": float(n_test),\n",
    "        \"num_centroids\": float(n_centroids),\n",
    "        \"num_distance_calls\": float(num_distance_calls),\n",
    "    }\n",
    "\n",
    "    if compute_resources:\n",
    "        res = estimate_resources_per_distance_call(msize)\n",
    "        metrics.update(res)\n",
    "\n",
    "    return y_pred, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59387600",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    X_scaled, y_full, feature_names, target_names = load_iris_two_features_scaled()\n",
    "    centroids, centroid_labels = compute_class_centroids(X_scaled, y_full)\n",
    "\n",
    "    print(\"Centroids (scaled [0,1], classical):\")\n",
    "    for label, mu in zip(centroid_labels, centroids):\n",
    "        print(\n",
    "            f\"  class {label} ({target_names[label]}): \"\n",
    "            f\"{feature_names[0]}={mu[0]:.4f}, {feature_names[1]}={mu[1]:.4f}\"\n",
    "        )\n",
    "\n",
    "    msizes = [3, 4, 5, 6, 7, 8]\n",
    "\n",
    "    metrics_rows: List[Dict[str, float]] = []\n",
    "    pred_df = pd.DataFrame(\n",
    "        {\n",
    "            \"sample_index\": np.arange(len(y_full), dtype=int),\n",
    "            \"true_label\": y_full.astype(int),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    for msize in msizes:\n",
    "        print(\n",
    "            f\"\\nRunning centroid classifier (Hamming) with QuantumFloat msize={msize}\"\n",
    "        )\n",
    "\n",
    "        y_pred, metrics = centroid_nn_hamming(\n",
    "            X_test=X_scaled,\n",
    "            centroids=centroids,\n",
    "            centroid_labels=centroid_labels,\n",
    "            msize=msize,\n",
    "            shots=512,\n",
    "            debug=True,\n",
    "            compute_resources=True,\n",
    "        )\n",
    "\n",
    "        acc = accuracy_score(y_full, y_pred)\n",
    "        cm = confusion_matrix(y_full, y_pred, labels=[0, 1, 2])\n",
    "\n",
    "        print(f\"  Accuracy (msize={msize}): {acc:.3f}\")\n",
    "        print(\"  Confusion matrix (rows=true, cols=pred):\")\n",
    "        print(cm)\n",
    "\n",
    "        metrics_row: Dict[str, float] = {\n",
    "            \"msize\": float(msize),\n",
    "            \"accuracy\": float(acc),\n",
    "        }\n",
    "        metrics_row.update(metrics)\n",
    "        metrics_rows.append(metrics_row)\n",
    "\n",
    "        cm_df = pd.DataFrame(\n",
    "            cm,\n",
    "            index=[f\"true_{cls}\" for cls in target_names],\n",
    "            columns=[f\"pred_{cls}\" for cls in target_names],\n",
    "        )\n",
    "        cm_path = os.path.join(BASE_OUT_DIR, f\"confusion_centroid_precision_{msize}.csv\")\n",
    "        cm_df.to_csv(cm_path, float_format=\"%.6e\")\n",
    "\n",
    "        pred_df[f\"pred_m{msize}\"] = y_pred.astype(int)\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics_rows)\n",
    "    metrics_path = os.path.join(BASE_OUT_DIR, \"centroid_metrics_and_resources_by_precision.csv\")\n",
    "    metrics_df.to_csv(metrics_path, index=False, float_format=\"%.6e\")\n",
    "\n",
    "    preds_path = os.path.join(BASE_OUT_DIR, \"centroid_predictions_by_precision.csv\")\n",
    "    pred_df.to_csv(preds_path, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
